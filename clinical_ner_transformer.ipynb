{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# clinical ner transformer\n",
    "\n",
    "In this notebook I use pretrained clinic_bioBert encoder transformer to extract diagnosis and medications (NER and classification) from medical notes data."
   ],
   "id": "e0a749b98971f74e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Main Objectives\n",
    "\n",
    "1. Generate or Load Data\n",
    "\n",
    "- Option A: Synthetic EMRs (fast to iterate; no licensing).\n",
    "- Option B: Open datasets (stronger realism): BC5CDR (Diseases/Chemicals): https://huggingface.co/datasets/biocreative_cdr\n",
    "\n",
    "2. BIO Tag the Data\n",
    "\n",
    "- Label scheme: B-ENTITY, I-ENTITY, O(no entity).\n",
    "\n",
    "3. Tokenize\n",
    "\n",
    "- Use the same tokenizer as the model (handles subwords + offset mapping).\n",
    "\n",
    "4. Load Pretrained Model\n",
    "\n",
    "- Bio_ClinicalBERT: emilyalsentzer/Bio_ClinicalBERT\n",
    "\n",
    "5. Fine-Tune\n",
    "\n",
    "- Objective: add token classification head (NER).\n",
    "- Loss: cross-entropy over token labels (ignore specials with -100).\n",
    "\n",
    "6. Classify Diagnoses & Meds\n",
    "\n",
    "- Postprocess logits - entity spans.\n",
    "- Aggregate subwords back to words; merge B-/I- runs.\n",
    "\n",
    "7. Evaluate\n",
    "\n",
    "- Metrics: precision / recall / F1 (seqeval).\n",
    "- Inspect errors (boundary splits, abbreviations, synonyms)."
   ],
   "id": "d198ea6b16ba8cc7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### install and import necessary modules",
   "id": "1a5640b9ea069bf1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!conda install -y pytorch cpuonly -c pytorch",
   "id": "316d6f4eb27a288a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-31T13:21:05.123597Z",
     "start_time": "2025-08-31T13:20:58.210374Z"
    }
   },
   "source": [
    "!pip install -U transformers seqeval evaluate bioc\n",
    "!pip install \"datasets<4.0.0\" fsspec pyarrow"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (4.56.0)\n",
      "Requirement already satisfied: seqeval in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: evaluate in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (0.4.5)\n",
      "Requirement already satisfied: bioc in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (2.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from transformers) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from transformers) (2025.8.29)\n",
      "Requirement already satisfied: requests in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from seqeval) (1.7.1)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: dill in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from evaluate) (2.3.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: lxml>=4.6.3 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from bioc) (6.0.1)\n",
      "Requirement already satisfied: jsonlines>=1.2.0 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from bioc) (4.0.0)\n",
      "Requirement already satisfied: intervaltree in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from bioc) (3.1.0)\n",
      "Requirement already satisfied: docopt in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from bioc) (0.6.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from datasets>=2.0.0->evaluate) (21.0.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (3.7)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from intervaltree->bioc) (2.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Using cached huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "Installing collected packages: huggingface-hub\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.25.2\n",
      "    Uninstalling huggingface-hub-0.25.2:\n",
      "      Successfully uninstalled huggingface-hub-0.25.2\n",
      "Successfully installed huggingface-hub-0.34.4\n",
      "Requirement already satisfied: datasets<4.0.0 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (2025.3.0)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (21.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from datasets<4.0.0) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from datasets<4.0.0) (2.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from datasets<4.0.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from datasets<4.0.0) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from datasets<4.0.0) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from datasets<4.0.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from datasets<4.0.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from datasets<4.0.0) (0.70.16)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from datasets<4.0.0) (0.34.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from datasets<4.0.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from datasets<4.0.0) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0) (3.12.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0) (3.7)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from requests>=2.32.2->datasets<4.0.0) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from requests>=2.32.2->datasets<4.0.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from requests>=2.32.2->datasets<4.0.0) (2025.8.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from tqdm>=4.66.3->datasets<4.0.0) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from pandas->datasets<4.0.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from pandas->datasets<4.0.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from pandas->datasets<4.0.0) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets<4.0.0) (1.17.0)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T13:21:19.400459Z",
     "start_time": "2025-08-31T13:21:14.698756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import transformers\n",
    "from datasets import load_dataset"
   ],
   "id": "132ae5c442e2a765",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T13:21:23.943507Z",
     "start_time": "2025-08-31T13:21:23.929345Z"
    }
   },
   "cell_type": "code",
   "source": "print(transformers.__version__)",
   "id": "97e92e26d95fc32f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.56.0\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### load dataset from hugging face datasets\n",
    "\n",
    "The BioCreative V Chemical Disease Relation (CDR) dataset is a large annotated text corpus of human annotations of all chemicals, diseases and their interactions in 1,500 PubMed articles."
   ],
   "id": "2d3f928497bad5d2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T09:27:18.378270Z",
     "start_time": "2025-09-01T09:27:16.050818Z"
    }
   },
   "cell_type": "code",
   "source": "dataset = load_dataset(\"bigbio/bc5cdr\", \"bc5cdr_source\", trust_remote_code=True)    ",
   "id": "8e0960178a79c0ba",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T09:27:26.266699Z",
     "start_time": "2025-09-01T09:27:26.240478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# dataset content\n",
    "dataset"
   ],
   "id": "81d1c1a75d722380",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['passages'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['passages'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['passages'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### observe dataset features and structure - \n",
   "id": "9fb790e654181024"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Each row in train / test / validation contains one document.\n",
    "Inside it, the passages field is a list of two items:\n",
    "\n",
    "- Title (type = \"title\")\n",
    "\n",
    "- Abstract (type = \"abstract\")\n",
    "\n",
    "Each passage has:\n",
    "\n",
    "- text: the actual string\n",
    "\n",
    "- entities: list of labeled entities (with offsets + text + type + normalized DB links)\n",
    "\n",
    "- relations: relations between entities (e.g. chemical–disease links)"
   ],
   "id": "df96fad1d2e51cb6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T09:27:32.922395Z",
     "start_time": "2025-09-01T09:27:32.911282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "passage_feature = dataset[\"train\"].features['passages']\n",
    "passage_feature"
   ],
   "id": "6268711c4e004bea",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'document_id': Value(dtype='string', id=None),\n",
       "  'type': Value(dtype='string', id=None),\n",
       "  'text': Value(dtype='string', id=None),\n",
       "  'entities': [{'id': Value(dtype='string', id=None),\n",
       "    'offsets': [[Value(dtype='int32', id=None)]],\n",
       "    'text': [Value(dtype='string', id=None)],\n",
       "    'type': Value(dtype='string', id=None),\n",
       "    'normalized': [{'db_name': Value(dtype='string', id=None),\n",
       "      'db_id': Value(dtype='string', id=None)}]}],\n",
       "  'relations': [{'id': Value(dtype='string', id=None),\n",
       "    'type': Value(dtype='string', id=None),\n",
       "    'arg1_id': Value(dtype='string', id=None),\n",
       "    'arg2_id': Value(dtype='string', id=None)}]}]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "random example from dataset ",
   "id": "d029be321577162b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T09:27:38.060330Z",
     "start_time": "2025-09-01T09:27:38.037133Z"
    }
   },
   "cell_type": "code",
   "source": "dataset[\"train\"][200] ",
   "id": "51dfa37eb4627414",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'passages': [{'document_id': '6747681',\n",
       "   'type': 'title',\n",
       "   'text': 'Intra-arterial BCNU chemotherapy for treatment of malignant gliomas of the central nervous system.',\n",
       "   'entities': [{'id': '0',\n",
       "     'offsets': [[15, 19]],\n",
       "     'text': ['BCNU'],\n",
       "     'type': 'Chemical',\n",
       "     'normalized': [{'db_name': 'MESH', 'db_id': 'D002330'}]},\n",
       "    {'id': '1',\n",
       "     'offsets': [[50, 67]],\n",
       "     'text': ['malignant gliomas'],\n",
       "     'type': 'Disease',\n",
       "     'normalized': [{'db_name': 'MESH', 'db_id': 'D005910'}]}],\n",
       "   'relations': [{'id': 'R0',\n",
       "     'type': 'CID',\n",
       "     'arg1_id': 'D002330',\n",
       "     'arg2_id': 'D031300'}]},\n",
       "  {'document_id': '6747681',\n",
       "   'type': 'abstract',\n",
       "   'text': 'Because of the rapid systemic clearance of BCNU (1,3-bis-(2-chloroethyl)-1-nitrosourea), intra-arterial administration should provide a substantial advantage over intravenous administration for the treatment of malignant gliomas. Thirty-six patients were treated with BCNU every 6 to 8 weeks, either by transfemoral catheterization of the internal carotid or vertebral artery or through a fully implantable intracarotid drug delivery system, beginning with a dose of 200 mg/sq m body surface area. Twelve patients with Grade III or IV astrocytomas were treated after partial resection of the tumor without prior radiation therapy. After two to seven cycles of chemotherapy, nine patients showed a decrease in tumor size and surrounding edema on contrast-enhanced computerized tomography scans. In the nine responders, median duration of chemotherapy response from the time of operation was 25 weeks (range 12 to more than 91 weeks). The median duration of survival in the 12 patients was 54 weeks (range 21 to more than 156 weeks), with an 18-month survival rate of 42%. Twenty-four patients with recurrent Grade I to IV astrocytomas, whose resection and irradiation therapy had failed, received two to eight courses of intra-arterial BCNU therapy. Seventeen of these had a response or were stable for a median of 20 weeks (range 6 to more than 66 weeks). The catheterization procedure is safe, with no immediate complication in 111 infusions of BCNU. A delayed complication in nine patients has been unilateral loss of vision secondary to a retinal vasculitis. The frequency of visual loss decreased after the concentration of the ethanol diluent was lowered.',\n",
       "   'entities': [{'id': '2',\n",
       "     'offsets': [[142, 146]],\n",
       "     'text': ['BCNU'],\n",
       "     'type': 'Chemical',\n",
       "     'normalized': [{'db_name': 'MESH', 'db_id': 'D002330'}]},\n",
       "    {'id': '3',\n",
       "     'offsets': [[148, 185]],\n",
       "     'text': ['1,3-bis-(2-chloroethyl)-1-nitrosourea'],\n",
       "     'type': 'Chemical',\n",
       "     'normalized': [{'db_name': 'MESH', 'db_id': 'D002330'}]},\n",
       "    {'id': '4',\n",
       "     'offsets': [[310, 327]],\n",
       "     'text': ['malignant gliomas'],\n",
       "     'type': 'Disease',\n",
       "     'normalized': [{'db_name': 'MESH', 'db_id': 'D005910'}]},\n",
       "    {'id': '5',\n",
       "     'offsets': [[367, 371]],\n",
       "     'text': ['BCNU'],\n",
       "     'type': 'Chemical',\n",
       "     'normalized': [{'db_name': 'MESH', 'db_id': 'D002330'}]},\n",
       "    {'id': '6',\n",
       "     'offsets': [[634, 646]],\n",
       "     'text': ['astrocytomas'],\n",
       "     'type': 'Disease',\n",
       "     'normalized': [{'db_name': 'MESH', 'db_id': 'D001254'}]},\n",
       "    {'id': '7',\n",
       "     'offsets': [[691, 696]],\n",
       "     'text': ['tumor'],\n",
       "     'type': 'Disease',\n",
       "     'normalized': [{'db_name': 'MESH', 'db_id': 'D009369'}]},\n",
       "    {'id': '8',\n",
       "     'offsets': [[808, 813]],\n",
       "     'text': ['tumor'],\n",
       "     'type': 'Disease',\n",
       "     'normalized': [{'db_name': 'MESH', 'db_id': 'D009369'}]},\n",
       "    {'id': '9',\n",
       "     'offsets': [[835, 840]],\n",
       "     'text': ['edema'],\n",
       "     'type': 'Disease',\n",
       "     'normalized': [{'db_name': 'MESH', 'db_id': 'D004487'}]},\n",
       "    {'id': '10',\n",
       "     'offsets': [[1220, 1232]],\n",
       "     'text': ['astrocytomas'],\n",
       "     'type': 'Disease',\n",
       "     'normalized': [{'db_name': 'MESH', 'db_id': 'D001254'}]},\n",
       "    {'id': '11',\n",
       "     'offsets': [[1334, 1338]],\n",
       "     'text': ['BCNU'],\n",
       "     'type': 'Chemical',\n",
       "     'normalized': [{'db_name': 'MESH', 'db_id': 'D002330'}]},\n",
       "    {'id': '12',\n",
       "     'offsets': [[1545, 1549]],\n",
       "     'text': ['BCNU'],\n",
       "     'type': 'Chemical',\n",
       "     'normalized': [{'db_name': 'MESH', 'db_id': 'D002330'}]},\n",
       "    {'id': '13',\n",
       "     'offsets': [[1611, 1625]],\n",
       "     'text': ['loss of vision'],\n",
       "     'type': 'Disease',\n",
       "     'normalized': [{'db_name': 'MESH', 'db_id': 'D014786'}]},\n",
       "    {'id': '14',\n",
       "     'offsets': [[1641, 1659]],\n",
       "     'text': ['retinal vasculitis'],\n",
       "     'type': 'Disease',\n",
       "     'normalized': [{'db_name': 'MESH', 'db_id': 'D031300'}]},\n",
       "    {'id': '15',\n",
       "     'offsets': [[1678, 1689]],\n",
       "     'text': ['visual loss'],\n",
       "     'type': 'Disease',\n",
       "     'normalized': [{'db_name': 'MESH', 'db_id': 'D014786'}]},\n",
       "    {'id': '16',\n",
       "     'offsets': [[1731, 1738]],\n",
       "     'text': ['ethanol'],\n",
       "     'type': 'Chemical',\n",
       "     'normalized': [{'db_name': 'MESH', 'db_id': 'D000431'}]}],\n",
       "   'relations': [{'id': 'R0',\n",
       "     'type': 'CID',\n",
       "     'arg1_id': 'D002330',\n",
       "     'arg2_id': 'D031300'}]}]}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Build BIO tags dataset\n",
    "\n",
    "For each entity offset, mark the corresponding tokens with B-(TYPE) and I-(TYPE).\n",
    "This converts the BigBio dataset into a standard token classification dataset."
   ],
   "id": "66d3cc6eb5ee4e6b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T09:28:54.902192Z",
     "start_time": "2025-09-01T09:28:54.870558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "label_lst = [\"O\",\"B-Chemical\",\"I-Chemical\",\"B-Disease\",\"I-Disease\"]\n",
    "label_to_id = {l:i for i,l in enumerate(label_lst)}\n",
    "\n",
    "def separate_to_words(text: str):\n",
    "    # get text and return a separated words list with their start and end indexes\n",
    "    # output shape: List[Tuple[str,int,int]]\n",
    "    return [(m.group(0), m.start(), m.end()) for m in re.finditer(r\"[A-Za-z]+(?:'[A-Za-z]+)?|[^\\w\\s]\", text)]\n",
    "\n",
    "def get_entities(passage: Dict):\n",
    "    # get features dictionary and return an entity type list with their start and end indexes\n",
    "    # output shape: List[Tuple[str,int,int]]\n",
    "    spans = []\n",
    "    for ent in passage.get(\"entities\", []):\n",
    "        # BigBio offsets: [[start, end]], extract min and max\n",
    "        offs = ent.get(\"offsets\", [])\n",
    "        if not offs: \n",
    "            continue\n",
    "        s = min(o[0] for o in offs)\n",
    "        e = max(o[1] for o in offs)\n",
    "        t = ent.get(\"type\", \"\")\n",
    "        spans.append((s, e, t))\n",
    "    return spans\n",
    "\n",
    "def merge_passages(passages: List[Dict], joiner=\" \"):\n",
    "    # get a dict that contains two passages and merge them. return the full text and its entities\n",
    "    # output shape: text, List[Tuple[int,int,str]]\n",
    "    full, ents, offset = [], [], 0\n",
    "    for p in passages:\n",
    "        text = p.get(\"text\",\"\")\n",
    "        full.append(text)\n",
    "        full.append(joiner)  # add white space between title and abstract passages\n",
    "        for (s,e,t) in get_entities(p):\n",
    "            ents.append((s+offset, e+offset, t))\n",
    "        offset += len(text) + len(joiner)\n",
    "    if full and full[-1] == joiner:\n",
    "        full.pop()  # drop redundant whitespace\n",
    " \n",
    "    return \"\".join(full), ents\n",
    "\n",
    "def main_doc_to_words_tags(doc: Dict) -> Dict[str, List]:\n",
    "    text, entity_spans = merge_passages(doc[\"passages\"])\n",
    "    toks = separate_to_words(text)\n",
    "    words  = [w for (w,_,_) in toks]\n",
    "    tags   = [\"O\"] * len(words)\n",
    "\n",
    "    # map BigBio types to two classes\n",
    "    def map_type(t: str):\n",
    "        t = t.lower()\n",
    "        if t.startswith(\"chem\"): \n",
    "            return \"Chemical\"\n",
    "        if t.startswith(\"dis\"):  \n",
    "            return \"Disease\"\n",
    "        return None\n",
    "\n",
    "    # assign BIO by overlap and continuity\n",
    "    for i, (tok, s, e) in enumerate(toks):\n",
    "        # find overlapping entity with max overlap\n",
    "        best = None; best_ov = 0\n",
    "        for (es, ee, t) in entity_spans:\n",
    "            ov = min(e, ee) - max(s, es)\n",
    "            if ov > best_ov:\n",
    "                best_ov = ov\n",
    "                best = (es, ee, t)\n",
    "        if best and best_ov > 0:\n",
    "            es, ee, t = best\n",
    "            cls = map_type(t)\n",
    "            if cls:\n",
    "                # B- if token starts inside entity at or before entity start; else I-\n",
    "                tags[i] = \"B-\"+cls if s <= es < e else \"I-\"+cls\n",
    "\n",
    "    # enforce I- continuity\n",
    "    for i in range(len(tags)):\n",
    "        if tags[i].startswith(\"I-\"):\n",
    "            if i == 0 or tags[i-1] == \"O\" or tags[i-1][2:] != tags[i][2:]:\n",
    "                tags[i] = \"B-\" + tags[i][2:]\n",
    "\n",
    "    ner_ids = [label_to_id.get(t, 0) for t in tags]\n",
    "    return {\"words\": words, \"ner_tags\": ner_ids}"
   ],
   "id": "1c8521b645024fe7",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T09:29:01.411274Z",
     "start_time": "2025-09-01T09:28:57.138925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import Dataset, DatasetDict, Features, Sequence, Value, ClassLabel\n",
    "\n",
    "processed = {}\n",
    "for split in dataset.keys():  # train test and val\n",
    "    rows = [main_doc_to_words_tags(doc) for doc in dataset[split]]\n",
    "    ds_split = Dataset.from_list(rows)\n",
    "    feats = Features({\n",
    "        \"words\": Sequence(Value(\"string\")),\n",
    "        \"ner_tags\": Sequence(ClassLabel(names=label_lst))\n",
    "    })\n",
    "    processed[split] = ds_split.cast(feats)\n",
    "\n",
    "token_level_ds = DatasetDict(processed)\n",
    "print(token_level_ds)"
   ],
   "id": "cb9bc2a9b4c9f04",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Casting the dataset:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d2efdbf52cff4c30bfaac7dcb2a57c3b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Casting the dataset:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "073fcdb12e7f4c9f89fcd7b94f62ef7d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Casting the dataset:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6a57800402464afd8e5fec69907560be"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['words', 'ner_tags'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['words', 'ner_tags'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['words', 'ner_tags'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "present some examples of post bio tag dataset",
   "id": "6dd02d8ae5ac320"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T09:29:15.238190Z",
     "start_time": "2025-09-01T09:29:15.223049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "words = token_level_ds[\"train\"][200][\"words\"][:20]\n",
    "labels = token_level_ds[\"train\"][200][\"ner_tags\"][:20]\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_lst[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "\n",
    "print(line1)\n",
    "print(line2)"
   ],
   "id": "322e0f947781748d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intra - arterial BCNU       chemotherapy for treatment of malignant gliomas   of the central nervous system . Because of the rapid \n",
      "O     O O        B-Chemical O            O   O         O  B-Disease I-Disease O  O   O       O       O      O O       O  O   O     \n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Apply Hugging Face tokenizer",
   "id": "7d29226b603f5bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T07:47:10.668114Z",
     "start_time": "2025-09-01T07:47:05.989784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
   ],
   "id": "f7d270707361bb01",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9fe2462e9a524c98aa2c7f1aeed0b457"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\shani\\.cache\\huggingface\\hub\\models--emilyalsentzer--Bio_ClinicalBERT. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f115a10b8bde4ff99a927d717fec59bb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T09:31:32.939416Z",
     "start_time": "2025-09-01T09:31:32.923985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_and_align_labels(example):\n",
    "    # tokenize words\n",
    "    tokenized = tokenizer(example[\"words\"], is_split_into_words=True, truncation=True)\n",
    "    word_ids = tokenized.word_ids()\n",
    "\n",
    "    aligned_labels = []\n",
    "    previous_word_id = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id is None:\n",
    "            # special tokens (CLS, SEP, PAD)\n",
    "            aligned_labels.append(-100)\n",
    "        elif word_id != previous_word_id:\n",
    "            # first subword → take label\n",
    "            aligned_labels.append(example[\"ner_tags\"][word_id])\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = example[\"ner_tags\"][word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            aligned_labels.append(label)\n",
    "        previous_word_id = word_id\n",
    "\n",
    "    tokenized[\"labels\"] = aligned_labels\n",
    "    return tokenized"
   ],
   "id": "91104cac8705c4d1",
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T09:31:38.676602Z",
     "start_time": "2025-09-01T09:31:34.640841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_datasets = token_level_ds.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=False,\n",
    "    remove_columns=token_level_ds[\"train\"].column_names\n",
    ")\n",
    "print(tokenized_datasets[\"train\"][0].keys())"
   ],
   "id": "7160b20cfb5e2539",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5a3f1204fac74bc3865b8495ceaca83e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cfd11e5b6dba4ef08266636202b29e57"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ef06d91ac455419f98baf437c28eaccd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T09:31:45.575413Z",
     "start_time": "2025-09-01T09:31:45.549724Z"
    }
   },
   "cell_type": "code",
   "source": "tokenized_datasets[\"train\"][200] ",
   "id": "5edb2b911a6def65",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101,\n",
       "  1107,\n",
       "  4487,\n",
       "  118,\n",
       "  1893,\n",
       "  19860,\n",
       "  171,\n",
       "  1665,\n",
       "  14787,\n",
       "  22572,\n",
       "  5521,\n",
       "  20939,\n",
       "  1111,\n",
       "  3252,\n",
       "  1104,\n",
       "  12477,\n",
       "  2646,\n",
       "  15454,\n",
       "  176,\n",
       "  9436,\n",
       "  7941,\n",
       "  1104,\n",
       "  1103,\n",
       "  2129,\n",
       "  5604,\n",
       "  1449,\n",
       "  119,\n",
       "  1272,\n",
       "  1104,\n",
       "  1103,\n",
       "  6099,\n",
       "  27410,\n",
       "  16443,\n",
       "  1104,\n",
       "  171,\n",
       "  1665,\n",
       "  14787,\n",
       "  113,\n",
       "  117,\n",
       "  118,\n",
       "  16516,\n",
       "  1116,\n",
       "  118,\n",
       "  113,\n",
       "  118,\n",
       "  22572,\n",
       "  10885,\n",
       "  7745,\n",
       "  15644,\n",
       "  1233,\n",
       "  114,\n",
       "  118,\n",
       "  118,\n",
       "  11437,\n",
       "  8005,\n",
       "  7301,\n",
       "  3313,\n",
       "  1161,\n",
       "  114,\n",
       "  117,\n",
       "  1107,\n",
       "  4487,\n",
       "  118,\n",
       "  1893,\n",
       "  19860,\n",
       "  3469,\n",
       "  1431,\n",
       "  2194,\n",
       "  170,\n",
       "  6432,\n",
       "  4316,\n",
       "  1166,\n",
       "  1107,\n",
       "  4487,\n",
       "  7912,\n",
       "  2285,\n",
       "  3469,\n",
       "  1111,\n",
       "  1103,\n",
       "  3252,\n",
       "  1104,\n",
       "  12477,\n",
       "  2646,\n",
       "  15454,\n",
       "  176,\n",
       "  9436,\n",
       "  7941,\n",
       "  119,\n",
       "  3961,\n",
       "  118,\n",
       "  1565,\n",
       "  4420,\n",
       "  1127,\n",
       "  5165,\n",
       "  1114,\n",
       "  171,\n",
       "  1665,\n",
       "  14787,\n",
       "  1451,\n",
       "  1106,\n",
       "  2277,\n",
       "  117,\n",
       "  1719,\n",
       "  1118,\n",
       "  14715,\n",
       "  8124,\n",
       "  26271,\n",
       "  1348,\n",
       "  5855,\n",
       "  4638,\n",
       "  2083,\n",
       "  2734,\n",
       "  1104,\n",
       "  1103,\n",
       "  4422,\n",
       "  1610,\n",
       "  3329,\n",
       "  2386,\n",
       "  1137,\n",
       "  1396,\n",
       "  22460,\n",
       "  6766,\n",
       "  1233,\n",
       "  18593,\n",
       "  1137,\n",
       "  1194,\n",
       "  170,\n",
       "  3106,\n",
       "  24034,\n",
       "  9180,\n",
       "  1895,\n",
       "  1107,\n",
       "  4487,\n",
       "  8766,\n",
       "  3329,\n",
       "  2386,\n",
       "  3850,\n",
       "  6779,\n",
       "  1449,\n",
       "  117,\n",
       "  2150,\n",
       "  1114,\n",
       "  170,\n",
       "  13753,\n",
       "  1104,\n",
       "  17713,\n",
       "  120,\n",
       "  4816,\n",
       "  182,\n",
       "  1404,\n",
       "  2473,\n",
       "  1298,\n",
       "  119,\n",
       "  4030,\n",
       "  4420,\n",
       "  1114,\n",
       "  3654,\n",
       "  25550,\n",
       "  1182,\n",
       "  1137,\n",
       "  178,\n",
       "  1964,\n",
       "  1112,\n",
       "  8005,\n",
       "  3457,\n",
       "  18778,\n",
       "  2225,\n",
       "  1127,\n",
       "  5165,\n",
       "  1170,\n",
       "  7597,\n",
       "  1231,\n",
       "  25461,\n",
       "  1104,\n",
       "  1103,\n",
       "  14601,\n",
       "  1443,\n",
       "  2988,\n",
       "  8432,\n",
       "  7606,\n",
       "  119,\n",
       "  1170,\n",
       "  1160,\n",
       "  1106,\n",
       "  1978,\n",
       "  13874,\n",
       "  1104,\n",
       "  22572,\n",
       "  5521,\n",
       "  20939,\n",
       "  117,\n",
       "  2551,\n",
       "  4420,\n",
       "  2799,\n",
       "  170,\n",
       "  9711,\n",
       "  1107,\n",
       "  14601,\n",
       "  2060,\n",
       "  1105,\n",
       "  3376,\n",
       "  5048,\n",
       "  14494,\n",
       "  1113,\n",
       "  5014,\n",
       "  118,\n",
       "  9927,\n",
       "  2775,\n",
       "  2200,\n",
       "  1106,\n",
       "  3702,\n",
       "  8944,\n",
       "  14884,\n",
       "  1116,\n",
       "  119,\n",
       "  1107,\n",
       "  1103,\n",
       "  2551,\n",
       "  6297,\n",
       "  1468,\n",
       "  117,\n",
       "  3151,\n",
       "  9355,\n",
       "  1104,\n",
       "  22572,\n",
       "  5521,\n",
       "  20939,\n",
       "  2593,\n",
       "  1121,\n",
       "  1103,\n",
       "  1159,\n",
       "  1104,\n",
       "  2805,\n",
       "  1108,\n",
       "  2277,\n",
       "  113,\n",
       "  2079,\n",
       "  1106,\n",
       "  1167,\n",
       "  1190,\n",
       "  2277,\n",
       "  114,\n",
       "  119,\n",
       "  1103,\n",
       "  3151,\n",
       "  9355,\n",
       "  1104,\n",
       "  8115,\n",
       "  1107,\n",
       "  1103,\n",
       "  4420,\n",
       "  1108,\n",
       "  2277,\n",
       "  113,\n",
       "  2079,\n",
       "  1106,\n",
       "  1167,\n",
       "  1190,\n",
       "  2277,\n",
       "  114,\n",
       "  117,\n",
       "  1114,\n",
       "  1126,\n",
       "  118,\n",
       "  2370,\n",
       "  8115,\n",
       "  2603,\n",
       "  1104,\n",
       "  110,\n",
       "  119,\n",
       "  2570,\n",
       "  118,\n",
       "  1300,\n",
       "  4420,\n",
       "  1114,\n",
       "  1231,\n",
       "  21754,\n",
       "  3654,\n",
       "  178,\n",
       "  1106,\n",
       "  178,\n",
       "  1964,\n",
       "  1112,\n",
       "  8005,\n",
       "  3457,\n",
       "  18778,\n",
       "  2225,\n",
       "  117,\n",
       "  2133,\n",
       "  1231,\n",
       "  25461,\n",
       "  1105,\n",
       "  178,\n",
       "  10582,\n",
       "  25971,\n",
       "  7606,\n",
       "  1125,\n",
       "  2604,\n",
       "  117,\n",
       "  1460,\n",
       "  1160,\n",
       "  1106,\n",
       "  2022,\n",
       "  4770,\n",
       "  1104,\n",
       "  1107,\n",
       "  4487,\n",
       "  118,\n",
       "  1893,\n",
       "  19860,\n",
       "  171,\n",
       "  1665,\n",
       "  14787,\n",
       "  7606,\n",
       "  119,\n",
       "  10439,\n",
       "  1104,\n",
       "  1292,\n",
       "  1125,\n",
       "  170,\n",
       "  2593,\n",
       "  1137,\n",
       "  1127,\n",
       "  6111,\n",
       "  1111,\n",
       "  170,\n",
       "  3151,\n",
       "  1104,\n",
       "  2277,\n",
       "  113,\n",
       "  2079,\n",
       "  1106,\n",
       "  1167,\n",
       "  1190,\n",
       "  2277,\n",
       "  114,\n",
       "  119,\n",
       "  1103,\n",
       "  5855,\n",
       "  4638,\n",
       "  2083,\n",
       "  2734,\n",
       "  7791,\n",
       "  1110,\n",
       "  2914,\n",
       "  117,\n",
       "  1114,\n",
       "  1185,\n",
       "  5670,\n",
       "  3254,\n",
       "  15534,\n",
       "  1107,\n",
       "  1107,\n",
       "  17149,\n",
       "  1116,\n",
       "  1104,\n",
       "  171,\n",
       "  1665,\n",
       "  14787,\n",
       "  119,\n",
       "  170,\n",
       "  8088,\n",
       "  3254,\n",
       "  15534,\n",
       "  1107,\n",
       "  2551,\n",
       "  4420,\n",
       "  1144,\n",
       "  1151,\n",
       "  8362,\n",
       "  8009,\n",
       "  16719,\n",
       "  2445,\n",
       "  1104,\n",
       "  4152,\n",
       "  3718,\n",
       "  1106,\n",
       "  170,\n",
       "  1231,\n",
       "  11681,\n",
       "  1233,\n",
       "  191,\n",
       "  2225,\n",
       "  10182,\n",
       "  12888,\n",
       "  1548,\n",
       "  119,\n",
       "  1103,\n",
       "  5625,\n",
       "  1104,\n",
       "  5173,\n",
       "  2445,\n",
       "  10558,\n",
       "  1170,\n",
       "  1103,\n",
       "  6256,\n",
       "  1104,\n",
       "  1103,\n",
       "  27236,\n",
       "  4267,\n",
       "  19224,\n",
       "  2227,\n",
       "  1108,\n",
       "  6069,\n",
       "  119,\n",
       "  102],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [-100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  -100]}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T09:31:58.807879Z",
     "start_time": "2025-09-01T09:31:58.794542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_datasets[\"train\"][200][\"input_ids\"])\n",
    "labels = tokenized_datasets[\"train\"][200][\"labels\"]\n",
    "\n",
    "for t, l in zip(tokens[:30], labels[:30]):\n",
    "    label_name = label_lst[l] if l != -100 else \"IGN\"\n",
    "    print(f\"{t:15} {label_name}\")\n"
   ],
   "id": "24786b46096b72e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           IGN\n",
      "in              O\n",
      "##tra           O\n",
      "-               O\n",
      "art             O\n",
      "##erial         O\n",
      "b               B-Chemical\n",
      "##c             I-Chemical\n",
      "##nu            I-Chemical\n",
      "ch              O\n",
      "##em            O\n",
      "##otherapy      O\n",
      "for             O\n",
      "treatment       O\n",
      "of              O\n",
      "ma              B-Disease\n",
      "##li            I-Disease\n",
      "##gnant         I-Disease\n",
      "g               I-Disease\n",
      "##lio           I-Disease\n",
      "##mas           I-Disease\n",
      "of              O\n",
      "the             O\n",
      "central         O\n",
      "nervous         O\n",
      "system          O\n",
      ".               O\n",
      "because         O\n",
      "of              O\n",
      "the             O\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Fine-tuning the model",
   "id": "6d7fc217186e895f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c48ec21efdb44462"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
