{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# clinical ner transformer\n",
    "\n",
    "In this notebook I use pretrained clinic_bioBert encoder transformer to extract diagnosis and medications (NER and classification) from medical notes data."
   ],
   "id": "e0a749b98971f74e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Main Objectives\n",
    "\n",
    "1. Generate or Load Data\n",
    "\n",
    "- Option A: Synthetic EMRs \n",
    "- Option B: Open datasets: BC5CDR (Diseases/Chemicals): https://huggingface.co/datasets/biocreative_cdr\n",
    "\n",
    "2. BIO Tag the Data\n",
    "\n",
    "- Label scheme: B-ENTITY, I-ENTITY, O(no entity).\n",
    "\n",
    "3. Tokenize\n",
    "\n",
    "- Use the same tokenizer as the model (handles subwords + offset mapping).\n",
    "\n",
    "4. Load Pretrained Model\n",
    "\n",
    "- Bio_ClinicalBERT: emilyalsentzer/Bio_ClinicalBERT\n",
    "\n",
    "5. Fine-Tune\n",
    "\n",
    "- Objective: add token classification head (NER).\n",
    "- Loss: cross-entropy over token labels (ignore specials with -100).\n",
    "\n",
    "6. Classify Diagnoses & Meds\n",
    "\n",
    "- Postprocess logits - entity spans.\n",
    "- Aggregate subwords back to words; merge B-/I- runs.\n",
    "\n",
    "7. Evaluate\n",
    "\n",
    "- Metrics: precision / recall / F1 (seqeval).\n",
    "- Inspect errors (boundary splits, abbreviations, synonyms)."
   ],
   "id": "d198ea6b16ba8cc7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### install and import necessary modules",
   "id": "1a5640b9ea069bf1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!conda install -y pytorch cpuonly -c pytorch",
   "id": "316d6f4eb27a288a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install -U transformers seqeval evaluate bioc\n",
    "!pip install \"datasets<4.0.0\" fsspec pyarrow"
   ],
   "id": "78dbe0cde70c6874"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T09:34:51.577432Z",
     "start_time": "2025-09-02T09:34:48.314298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import transformers\n",
    "from datasets import load_dataset\n",
    "import torch"
   ],
   "id": "132ae5c442e2a765",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T09:34:55.188190Z",
     "start_time": "2025-09-02T09:34:55.172444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"torch:\", torch.__version__)"
   ],
   "id": "9950481252403b81",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers: 4.56.0\n",
      "torch: 2.6.0\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### load dataset from hugging face datasets\n",
    "\n",
    "The BioCreative V Chemical Disease Relation (CDR) dataset is a large annotated text corpus of human annotations of all chemicals, diseases and their interactions in 1,500 PubMed articles."
   ],
   "id": "2d3f928497bad5d2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T09:35:01.863807Z",
     "start_time": "2025-09-02T09:34:59.348830Z"
    }
   },
   "cell_type": "code",
   "source": "dataset = load_dataset(\"bigbio/bc5cdr\", \"bc5cdr_source\", trust_remote_code=True)    ",
   "id": "8e0960178a79c0ba",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T09:27:26.266699Z",
     "start_time": "2025-09-01T09:27:26.240478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# dataset content\n",
    "dataset"
   ],
   "id": "81d1c1a75d722380",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['passages'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['passages'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['passages'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### observe dataset features and structure - \n",
   "id": "9fb790e654181024"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Each row in train / test / validation contains one document.\n",
    "Inside it, the passages field is a list of two items:\n",
    "\n",
    "- Title (type = \"title\")\n",
    "\n",
    "- Abstract (type = \"abstract\")\n",
    "\n",
    "Each passage has:\n",
    "\n",
    "- text: the actual string\n",
    "\n",
    "- entities: list of labeled entities (with offsets + text + type + normalized DB links)\n",
    "\n",
    "- relations: relations between entities (e.g. chemical–disease links)"
   ],
   "id": "df96fad1d2e51cb6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T09:27:32.922395Z",
     "start_time": "2025-09-01T09:27:32.911282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "passage_feature = dataset[\"train\"].features['passages']\n",
    "passage_feature"
   ],
   "id": "6268711c4e004bea",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'document_id': Value(dtype='string', id=None),\n",
       "  'type': Value(dtype='string', id=None),\n",
       "  'text': Value(dtype='string', id=None),\n",
       "  'entities': [{'id': Value(dtype='string', id=None),\n",
       "    'offsets': [[Value(dtype='int32', id=None)]],\n",
       "    'text': [Value(dtype='string', id=None)],\n",
       "    'type': Value(dtype='string', id=None),\n",
       "    'normalized': [{'db_name': Value(dtype='string', id=None),\n",
       "      'db_id': Value(dtype='string', id=None)}]}],\n",
       "  'relations': [{'id': Value(dtype='string', id=None),\n",
       "    'type': Value(dtype='string', id=None),\n",
       "    'arg1_id': Value(dtype='string', id=None),\n",
       "    'arg2_id': Value(dtype='string', id=None)}]}]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "random example from dataset ",
   "id": "d029be321577162b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T09:27:38.060330Z",
     "start_time": "2025-09-01T09:27:38.037133Z"
    }
   },
   "cell_type": "code",
   "source": "dataset[\"train\"][200] ",
   "id": "51dfa37eb4627414",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'passages': [{'document_id': '6747681',\n",
       "   'type': 'title',\n",
       "   'text': 'Intra-arterial BCNU chemotherapy for treatment of malignant gliomas of the central nervous system.',\n",
       "   'entities': [{'id': '0',\n",
       "     'offsets': [[15, 19]],\n",
       "     'text': ['BCNU'],\n",
       "     'type': 'Chemical',\n",
       "     'normalized': [{'db_name': 'MESH', 'db_id': 'D002330'}]},\n",
       "    {'id': '1',\n",
       "     'offsets': [[50, 67]],\n",
       "     'text': ['malignant gliomas'],\n",
       "     'type': 'Disease',\n",
       "     'normalized': [{'db_name': 'MESH', 'db_id': 'D005910'}]}],\n",
       "   'relations': [{'id': 'R0',\n",
       "     'type': 'CID',\n",
       "     'arg1_id': 'D002330',\n",
       "     'arg2_id': 'D031300'}]},\n",
       "  {'document_id': '6747681',\n",
       "   'type': 'abstract',\n",
       "   'text': 'Because of the rapid systemic clearance of BCNU (1,3-bis-(2-chloroethyl)-1-nitrosourea), intra-arterial administration should provide a substantial advantage over intravenous administration for the treatment of malignant gliomas. Thirty-six patients were treated with BCNU every 6 to 8 weeks, either by transfemoral catheterization of the internal carotid or vertebral artery or through a fully implantable intracarotid drug delivery system, beginning with a dose of 200 mg/sq m body surface area. Twelve patients with Grade III or IV astrocytomas were treated after partial resection of the tumor without prior radiation therapy. After two to seven cycles of chemotherapy, nine patients showed a decrease in tumor size and surrounding edema on contrast-enhanced computerized tomography scans. In the nine responders, median duration of chemotherapy response from the time of operation was 25 weeks (range 12 to more than 91 weeks). The median duration of survival in the 12 patients was 54 weeks (range 21 to more than 156 weeks), with an 18-month survival rate of 42%. Twenty-four patients with recurrent Grade I to IV astrocytomas, whose resection and irradiation therapy had failed, received two to eight courses of intra-arterial BCNU therapy. Seventeen of these had a response or were stable for a median of 20 weeks (range 6 to more than 66 weeks). The catheterization procedure is safe, with no immediate complication in 111 infusions of BCNU. A delayed complication in nine patients has been unilateral loss of vision secondary to a retinal vasculitis. The frequency of visual loss decreased after the concentration of the ethanol diluent was lowered.',\n",
       "   'entities': [{'id': '2',\n",
       "     'offsets': [[142, 146]],\n",
       "     'text': ['BCNU'],\n",
       "     'type': 'Chemical',\n",
       "     'normalized': [{'db_name': 'MESH', 'db_id': 'D002330'}]},\n",
       "    {'id': '3',\n",
       "     'offsets': [[148, 185]],\n",
       "     'text': ['1,3-bis-(2-chloroethyl)-1-nitrosourea'],\n",
       "     'type': 'Chemical',\n",
       "     'normalized': [{'db_name': 'MESH', 'db_id': 'D002330'}]},\n",
       "    {'id': '4',\n",
       "     'offsets': [[310, 327]],\n",
       "     'text': ['malignant gliomas'],\n",
       "     'type': 'Disease',\n",
       "     'normalized': [{'db_name': 'MESH', 'db_id': 'D005910'}]},\n",
       "    {'id': '5',\n",
       "     'offsets': [[367, 371]],\n",
       "     'text': ['BCNU'],\n",
       "     'type': 'Chemical',\n",
       "     'normalized': [{'db_name': 'MESH', 'db_id': 'D002330'}]},\n",
       "    {'id': '6',\n",
       "     'offsets': [[634, 646]],\n",
       "     'text': ['astrocytomas'],\n",
       "     'type': 'Disease',\n",
       "     'normalized': [{'db_name': 'MESH', 'db_id': 'D001254'}]},\n",
       "    {'id': '7',\n",
       "     'offsets': [[691, 696]],\n",
       "     'text': ['tumor'],\n",
       "     'type': 'Disease',\n",
       "     'normalized': [{'db_name': 'MESH', 'db_id': 'D009369'}]},\n",
       "    {'id': '8',\n",
       "     'offsets': [[808, 813]],\n",
       "     'text': ['tumor'],\n",
       "     'type': 'Disease',\n",
       "     'normalized': [{'db_name': 'MESH', 'db_id': 'D009369'}]},\n",
       "    {'id': '9',\n",
       "     'offsets': [[835, 840]],\n",
       "     'text': ['edema'],\n",
       "     'type': 'Disease',\n",
       "     'normalized': [{'db_name': 'MESH', 'db_id': 'D004487'}]},\n",
       "    {'id': '10',\n",
       "     'offsets': [[1220, 1232]],\n",
       "     'text': ['astrocytomas'],\n",
       "     'type': 'Disease',\n",
       "     'normalized': [{'db_name': 'MESH', 'db_id': 'D001254'}]},\n",
       "    {'id': '11',\n",
       "     'offsets': [[1334, 1338]],\n",
       "     'text': ['BCNU'],\n",
       "     'type': 'Chemical',\n",
       "     'normalized': [{'db_name': 'MESH', 'db_id': 'D002330'}]},\n",
       "    {'id': '12',\n",
       "     'offsets': [[1545, 1549]],\n",
       "     'text': ['BCNU'],\n",
       "     'type': 'Chemical',\n",
       "     'normalized': [{'db_name': 'MESH', 'db_id': 'D002330'}]},\n",
       "    {'id': '13',\n",
       "     'offsets': [[1611, 1625]],\n",
       "     'text': ['loss of vision'],\n",
       "     'type': 'Disease',\n",
       "     'normalized': [{'db_name': 'MESH', 'db_id': 'D014786'}]},\n",
       "    {'id': '14',\n",
       "     'offsets': [[1641, 1659]],\n",
       "     'text': ['retinal vasculitis'],\n",
       "     'type': 'Disease',\n",
       "     'normalized': [{'db_name': 'MESH', 'db_id': 'D031300'}]},\n",
       "    {'id': '15',\n",
       "     'offsets': [[1678, 1689]],\n",
       "     'text': ['visual loss'],\n",
       "     'type': 'Disease',\n",
       "     'normalized': [{'db_name': 'MESH', 'db_id': 'D014786'}]},\n",
       "    {'id': '16',\n",
       "     'offsets': [[1731, 1738]],\n",
       "     'text': ['ethanol'],\n",
       "     'type': 'Chemical',\n",
       "     'normalized': [{'db_name': 'MESH', 'db_id': 'D000431'}]}],\n",
       "   'relations': [{'id': 'R0',\n",
       "     'type': 'CID',\n",
       "     'arg1_id': 'D002330',\n",
       "     'arg2_id': 'D031300'}]}]}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Build BIO tags dataset\n",
    "\n",
    "For each entity offset, mark the corresponding tokens with B-(TYPE) and I-(TYPE).\n",
    "This converts the BigBio dataset into a standard token classification dataset."
   ],
   "id": "66d3cc6eb5ee4e6b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T09:35:08.544314Z",
     "start_time": "2025-09-02T09:35:08.520561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "label_lst = [\"O\",\"B-Chemical\",\"I-Chemical\",\"B-Disease\",\"I-Disease\"]\n",
    "label_to_id = {l:i for i,l in enumerate(label_lst)}\n",
    "\n",
    "def separate_to_words(text: str):\n",
    "    # get text and return a separated words list with their start and end indexes\n",
    "    # output shape: List[Tuple[str,int,int]]\n",
    "    return [(m.group(0), m.start(), m.end()) for m in re.finditer(r\"[A-Za-z]+(?:'[A-Za-z]+)?|[^\\w\\s]\", text)]\n",
    "\n",
    "def get_entities(passage: Dict):\n",
    "    # get features dictionary and return an entity type list with their start and end indexes\n",
    "    # output shape: List[Tuple[str,int,int]]\n",
    "    spans = []\n",
    "    for ent in passage.get(\"entities\", []):\n",
    "        # BigBio offsets: [[start, end]], extract min and max\n",
    "        offs = ent.get(\"offsets\", [])\n",
    "        if not offs: \n",
    "            continue\n",
    "        s = min(o[0] for o in offs)\n",
    "        e = max(o[1] for o in offs)\n",
    "        t = ent.get(\"type\", \"\")\n",
    "        spans.append((s, e, t))\n",
    "    return spans\n",
    "\n",
    "def merge_passages(passages: List[Dict], joiner=\" \"):\n",
    "    # get a dict that contains two passages and merge them. return the full text and its entities\n",
    "    # output shape: text, List[Tuple[int,int,str]]\n",
    "    full, ents, offset = [], [], 0\n",
    "    for p in passages:\n",
    "        text = p.get(\"text\",\"\")\n",
    "        full.append(text)\n",
    "        full.append(joiner)  # add white space between title and abstract passages\n",
    "        for (s,e,t) in get_entities(p):\n",
    "            ents.append((s+offset, e+offset, t))\n",
    "        offset += len(text) + len(joiner)\n",
    "    if full and full[-1] == joiner:\n",
    "        full.pop()  # drop redundant whitespace\n",
    " \n",
    "    return \"\".join(full), ents\n",
    "\n",
    "def main_doc_to_words_tags(doc: Dict) -> Dict[str, List]:\n",
    "    text, entity_spans = merge_passages(doc[\"passages\"])\n",
    "    toks = separate_to_words(text)\n",
    "    words  = [w for (w,_,_) in toks]\n",
    "    tags   = [\"O\"] * len(words)\n",
    "\n",
    "    # map BigBio types to two classes\n",
    "    def map_type(t: str):\n",
    "        t = t.lower()\n",
    "        if t.startswith(\"chem\"): \n",
    "            return \"Chemical\"\n",
    "        if t.startswith(\"dis\"):  \n",
    "            return \"Disease\"\n",
    "        return None\n",
    "\n",
    "    # assign BIO by overlap and continuity\n",
    "    for i, (tok, s, e) in enumerate(toks):\n",
    "        # find overlapping entity with max overlap\n",
    "        best = None; best_ov = 0\n",
    "        for (es, ee, t) in entity_spans:\n",
    "            ov = min(e, ee) - max(s, es)\n",
    "            if ov > best_ov:\n",
    "                best_ov = ov\n",
    "                best = (es, ee, t)\n",
    "        if best and best_ov > 0:\n",
    "            es, ee, t = best\n",
    "            cls = map_type(t)\n",
    "            if cls:\n",
    "                # B- if token starts inside entity at or before entity start; else I-\n",
    "                tags[i] = \"B-\"+cls if s <= es < e else \"I-\"+cls\n",
    "\n",
    "    # enforce I- continuity\n",
    "    for i in range(len(tags)):\n",
    "        if tags[i].startswith(\"I-\"):\n",
    "            if i == 0 or tags[i-1] == \"O\" or tags[i-1][2:] != tags[i][2:]:\n",
    "                tags[i] = \"B-\" + tags[i][2:]\n",
    "\n",
    "    ner_ids = [label_to_id.get(t, 0) for t in tags]\n",
    "    return {\"words\": words, \"ner_tags\": ner_ids}"
   ],
   "id": "1c8521b645024fe7",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T09:35:15.177496Z",
     "start_time": "2025-09-02T09:35:12.188149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import Dataset, DatasetDict, Features, Sequence, Value, ClassLabel\n",
    "\n",
    "processed = {}\n",
    "for split in dataset.keys():  # train test and val\n",
    "    rows = [main_doc_to_words_tags(doc) for doc in dataset[split]]\n",
    "    ds_split = Dataset.from_list(rows)\n",
    "    feats = Features({\n",
    "        \"words\": Sequence(Value(\"string\")),\n",
    "        \"ner_tags\": Sequence(ClassLabel(names=label_lst))\n",
    "    })\n",
    "    processed[split] = ds_split.cast(feats)\n",
    "\n",
    "token_level_ds = DatasetDict(processed)\n",
    "print(token_level_ds)"
   ],
   "id": "cb9bc2a9b4c9f04",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Casting the dataset:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1aef03b77cc247fb9809987c3b0ffeaa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Casting the dataset:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b993303bc3c943ab889b399528ae4ac2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Casting the dataset:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f2dbe059149949eea1961ca270a73046"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['words', 'ner_tags'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['words', 'ner_tags'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['words', 'ner_tags'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "present some examples of post bio tag dataset",
   "id": "6dd02d8ae5ac320"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T09:29:15.238190Z",
     "start_time": "2025-09-01T09:29:15.223049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "words = token_level_ds[\"train\"][200][\"words\"][:20]\n",
    "labels = token_level_ds[\"train\"][200][\"ner_tags\"][:20]\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_lst[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "\n",
    "print(line1)\n",
    "print(line2)"
   ],
   "id": "322e0f947781748d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intra - arterial BCNU       chemotherapy for treatment of malignant gliomas   of the central nervous system . Because of the rapid \n",
      "O     O O        B-Chemical O            O   O         O  B-Disease I-Disease O  O   O       O       O      O O       O  O   O     \n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Apply Hugging Face tokenizer",
   "id": "7d29226b603f5bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T09:35:23.362328Z",
     "start_time": "2025-09-02T09:35:19.331429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
   ],
   "id": "f7d270707361bb01",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T09:35:26.739941Z",
     "start_time": "2025-09-02T09:35:26.708358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_and_align_labels(example):\n",
    "    # tokenize words\n",
    "    tokenized = tokenizer(example[\"words\"], is_split_into_words=True, truncation=True)\n",
    "    word_ids = tokenized.word_ids()\n",
    "\n",
    "    aligned_labels = []\n",
    "    previous_word_id = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id is None:\n",
    "            # special tokens (CLS, SEP, PAD)\n",
    "            aligned_labels.append(-100)\n",
    "        elif word_id != previous_word_id:\n",
    "            # first subword → take label\n",
    "            aligned_labels.append(example[\"ner_tags\"][word_id])\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = example[\"ner_tags\"][word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            aligned_labels.append(label)\n",
    "        previous_word_id = word_id\n",
    "\n",
    "    tokenized[\"labels\"] = aligned_labels\n",
    "    return tokenized"
   ],
   "id": "91104cac8705c4d1",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T09:35:31.347160Z",
     "start_time": "2025-09-02T09:35:28.500217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_datasets = token_level_ds.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=False,\n",
    "    remove_columns=token_level_ds[\"train\"].column_names\n",
    ")\n",
    "print(tokenized_datasets[\"train\"][0].keys())"
   ],
   "id": "7160b20cfb5e2539",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "68d8de21c6b44096aa783085717689d3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "64a4c810d14c43db82d86acdab437e5a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2090deb3adb54e4586859e06bcff0170"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T09:21:13.859966Z",
     "start_time": "2025-09-02T09:21:13.837747Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_datasets[\"train\"][200][\"input_ids\"])\n",
    "labels = tokenized_datasets[\"train\"][200][\"labels\"]\n",
    "\n",
    "for t, l in zip(tokens[:30], labels[:30]):\n",
    "    label_name = label_lst[l] if l != -100 else \"IGN\"\n",
    "    print(f\"{t:15} {label_name}\")\n"
   ],
   "id": "24786b46096b72e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           IGN\n",
      "in              O\n",
      "##tra           O\n",
      "-               O\n",
      "art             O\n",
      "##erial         O\n",
      "b               B-Chemical\n",
      "##c             I-Chemical\n",
      "##nu            I-Chemical\n",
      "ch              O\n",
      "##em            O\n",
      "##otherapy      O\n",
      "for             O\n",
      "treatment       O\n",
      "of              O\n",
      "ma              B-Disease\n",
      "##li            I-Disease\n",
      "##gnant         I-Disease\n",
      "g               I-Disease\n",
      "##lio           I-Disease\n",
      "##mas           I-Disease\n",
      "of              O\n",
      "the             O\n",
      "central         O\n",
      "nervous         O\n",
      "system          O\n",
      ".               O\n",
      "because         O\n",
      "of              O\n",
      "the             O\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Fine-tuning the model",
   "id": "6d7fc217186e895f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### define a data collector\n",
    "\n",
    "pads input and labels of each batch to be the same length"
   ],
   "id": "3bc7a080f5e7cc9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T09:35:35.656515Z",
     "start_time": "2025-09-02T09:35:35.627612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ],
   "id": "518b756d714536d",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n",
    "batch[\"labels\"]"
   ],
   "id": "4eab6385b87dcade"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### define metrics using seqeval",
   "id": "d5d950fe8102d2ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T09:35:41.924680Z",
     "start_time": "2025-09-02T09:35:40.575705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_lst[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_lst[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ],
   "id": "4d1286d36b53c7a4",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### load pretrained model\n",
   "id": "c13734cbb2804f38"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T09:35:46.337130Z",
     "start_time": "2025-09-02T09:35:45.134581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "label_to_id = {l:i for i,l in enumerate(label_lst)}\n",
    "id_to_label = {i:l for i,l in enumerate(label_lst)}\n",
    "\n",
    "num_labels = len(label_lst)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"emilyalsentzer/Bio_ClinicalBERT\",\n",
    "    num_labels=num_labels,\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id\n",
    ")"
   ],
   "id": "9319fdaf1837efb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T07:43:41.435608Z",
     "start_time": "2025-09-02T07:43:41.413852Z"
    }
   },
   "cell_type": "code",
   "source": "model.config.num_labels",
   "id": "6f92419b69d08b95",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### define TrainingArguments and the trainer",
   "id": "82c64a50aa112a1d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T09:32:43.583357Z",
     "start_time": "2025-09-02T09:32:40.593175Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install -U \"accelerate>=0.26.0\"\n",
   "id": "20b886ceae72ac0f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate>=0.26.0\n",
      "  Downloading accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from accelerate>=0.26.0) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from accelerate>=0.26.0) (25.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from accelerate>=0.26.0) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from accelerate>=0.26.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from accelerate>=0.26.0) (2.6.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from accelerate>=0.26.0) (0.34.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from accelerate>=0.26.0) (0.6.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (2025.3.0)\n",
      "Requirement already satisfied: requests in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.26.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=0.26.0) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub>=0.21.0->accelerate>=0.26.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate>=0.26.0) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shani\\anaconda3\\envs\\medical_cv_projects\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (2025.8.3)\n",
      "Downloading accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.10.1\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T09:35:54.596034Z",
     "start_time": "2025-09-02T09:35:54.564341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"ner_clinicalbert_bc5cdr\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    seed=42,\n",
    "    fp16=torch.cuda.is_available()  # mixed precision if GPU\n",
    ")"
   ],
   "id": "dfd46794f8d6287",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T09:48:13.027740Z",
     "start_time": "2025-09-02T09:48:13.011087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ],
   "id": "7cd5631d20f6e932",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T08:29:53.488657Z",
     "start_time": "2025-09-02T08:29:53.477547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# custom training loop\n",
    "\n",
    "num_epochs = 3\n",
    "training_batch = 8\n",
    "for epoch in num_epochs:\n",
    "    "
   ],
   "id": "99d835653e23407a",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Fine-tune & Evaluate",
   "id": "c5efe6a2e3ce5a6f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "trainer.train()\n",
   "id": "ebfd09eaaea5735d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Validation metrics (best checkpoint already loaded if load_best_model_at_end=True)\n",
    "val_metrics = trainer.evaluate()\n",
    "print(\"Validation:\", val_metrics)\n",
    "\n",
    "# Test set evaluation\n",
    "test_metrics = trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])\n",
    "print(\"Test:\", test_metrics)"
   ],
   "id": "808bcdb7badf1cd2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Save the fine-tuned model (for FastAPI, batch jobs, etc.)",
   "id": "fab4199eebf58407"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "save_dir = \"ner_clinicalbert_bc5cdr_best\"\n",
    "trainer.model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "print(\"Saved to:\", save_dir)"
   ],
   "id": "712ecf2ccc49e49d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
